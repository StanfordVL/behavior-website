<div class="section features-3 my-5 fullWidth" style="background-image: url(' {{ '/assets/img/ill/p31.svg' | relative_url }}')">
  <div class="container">
    <div class="row text-center justify-content-center">
      <div class="col-lg-8">
        <h3 class="display-3 text-white">Embodiments, Actuation, Sensing</h3>
        <br>
      </div>
    </div>
    <div class="row justify-content-center align-items-center">
      <div class="col">
        <div class="info info-horizontal bg-white">
          <div class="icon icon-lg icon-shape icon-shape-primary shadow rounded-circle">
            <i class="ni ni-user-run text-info"></i>
          </div>
          <div class="description pl-4">
            <h6 class="title text-info">Embodiment</h6>
          
          <p class="description opacity-8">We provide three embodiments to evaluate your solutions in BEHAVIOR: a humanoid avatar used to collect human demonstrations in virtual reality, a Fetch mobile manipulator with one arm, and a Tiago bimanual mobile manipulator. You can also add your own robot!</p>
          <a href="https://stanfordvl.github.io/behavior/agents.html" class="text-info">Details of embodiment</a>
          </div>
        </div>
      </div>
      <div class="col">
        <img class="ml-lg-5" src="{{ '/assets/img/behavior/combo.jpg' | relative_url }}" height="250px" >
      </div>
      <div class="w-100"></div>
      <br>
      <br>
      <div class="col">
        <div class="info info-horizontal bg-white">
          <div class="icon icon-lg icon-shape icon-shape-primary shadow rounded-circle">
            <i class="ni ni-active-40 text-warning"></i>
            <!-- <i class="fa-solid fa-arrow-right-arrow-left"></i> -->
          </div>
          <div class="description pl-4">
            <h6 class="title text-warning">Actuation</h6>
          
            <p class="description opacity-8">Agents in BEHAVIOR must change the state of the world controlling navigation and manipulation at 30 Hz. We provide interfaces to navigate based on linear and angular velocities, and to control the arm(s) either in Cartesian or joint space. We also provide a set action primitives to be used or extended.</p>
            <a href="https://stanfordvl.github.io/behavior/agents.html#embodiments-actuation-sensing-grasping" class="text-warning">Details of actuation</a>
            </div>
        </div>
      </div>
      <div class="col">
        <video autoplay loop muted width="450" class="ml-lg-5">
              <source src="{{ '/assets/img/behavior/behav_web2.mp4' | relative_url }}" type="video/mp4">
              Sorry, your browser doesn't support embedded videos. 
            </video>
      </div>
      <div class="w-100"></div>
      <br>
      <br>
      <div class="col">
        <div class="info info-horizontal bg-white">
          <div class="icon icon-lg icon-shape icon-shape-primary shadow rounded-circle">
            <i class="ni ni-camera-compact text-warning"></i>
            <!-- <i class="fa-solid fa-arrow-right-arrow-left"></i> -->
          </div>
          <div class="description pl-4">
            <h6 class="title text-warning">Sensing and Perception</h6>
          
            <p class="description opacity-8">BEHAVIOR activities require to understand the scene, plan a strategy, and execute it based on the virtual sensor signals generated by onboard sensors. This includes visual signals (RGB-D images, segmentation...), and proprioception.</p>
            <a href="https://stanfordvl.github.io/behavior/agents.html#observations" class="text-warning">Details of sensing</a>
            </div>
        </div>
      </div>
      <div class="col">
        <img class="ml-lg-5" src="{{ '/assets/img/behavior/modalities3.jpg' | relative_url }}" height="220px" >
      </div>
    </div>
  </div>
</div>
