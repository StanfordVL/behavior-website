<div class="section features-3 my-5 fullWidth" style="background-image: url(' {{ '/assets/img/ill/p31.svg' | relative_url }}')">
  <div class="container">
    <div class="row text-center justify-content-center">
      <div class="col-lg-8">
        <h3 class="display-3 text-white">What makes BEHAVIOR different?</h3>
      </div>
    </div>
    <div class="row">
      <div class="col-md-4">
        <div class="info info-horizontal bg-white">
          <div class="icon icon-lg icon-shape icon-shape-primary shadow rounded-circle">
            <i class="ni ni-active-40 text-info"></i>
          </div>
          <div class="description pl-4">
            <h6 class="title text-info">100 Household Activities in Realistically Simulated Homes</h5>
          </div>
          <p class="description opacity-8">including cleaning, preparing food, tidying, polishing, installing elements, etc. The activities obtained from the American Time Use Survey and approximate the real distribution of tasks performed by humans in their everyday lives.</p>
          <span><a href="https://behavior.stanford.edu/activity_list.html" class="text-info">Task list</a> | <a href="https://behavior.stanford.edu/media/mosaic_behavior.jpg" class="text-info">Task images</a></span>
          <video autoplay loop muted width="200">
            <source src="{{ '/assets/img/behavior/behav_web1.mp4' | relative_url }}" type="video/mp4">
              Sorry, your browser doesn't support embedded videos.
            </video>
        </div>
      </div>
      <div class="col-md-4">
        <div class="info info-horizontal bg-white">
          <div class="icon icon-lg icon-shape icon-shape-primary shadow rounded-circle">
            <i class="ni ni-istanbul text-warning"></i>
            <!-- <i class="fa-solid fa-arrow-right-arrow-left"></i> -->
          </div>
          <div class="description pl-4">
            <h5 class="title text-warning">Decision Making based on Onboard Sensing for Navigation and Manipulation</h5>
            <p class="description opacity-8">the long-horizon activities require to understand the scene, plan a strategy and execute it controlling the motion of the embodied agent, all based on the virtual sensor signals generated by onboard sensors such as RGB-D cameras and position encoders; as close as it gets to the challenges of real-world.</p>
            <a href="https://docs.google.com/document/d/1u2m9Ld6Qo3eG-fvCzuAZN6lHxwpVVBlwLVdzo_WDlNI/edit?usp=sharing" class="text-warning">Benchmark instructions</a>
            <video autoplay loop muted width="200">
              <source src="{{ '/assets/img/behavior/behav_web2.mp4' | relative_url }}" type="video/mp4">
              Sorry, your browser doesn't support embedded videos. 
            </video>
          </div>
        </div>
      </div>
      <div class="col-md-4">
        <div class="info info-horizontal bg-white">
          <div class="icon icon-lg icon-shape icon-shape-primary shadow rounded-circle">
            <i class="ni ni-trophy text-danger"></i>
          </div>
          <div class="description pl-4">
            <h5 class="title text-danger">More Complex Interactions than just Pick-and-Place</h5>
            <p class="description opacity-8">accomplishing the BEHAVIOR activities require changing more than the position of the objects in the environment: they need to be cooked, frozen, soaked, cleaned, ... All these new types of state changes are supported by the provided simulator, iGibson 2.0, and enable completely new types of activities.</p>
            <a href="http://svl.stanford.edu/igibson/" class="text-danger">More about the simulator iGibson2</a>
            <video autoplay loop muted width="200">
              <source src="{{ '/assets/img/behavior/state_changes.mp4' | relative_url }}" type="video/mp4">Sorry, your browser doesn't support embedded videos.
              </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
